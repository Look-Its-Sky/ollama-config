name: Build and Deploy Ollama

on:
  push:
  workflow_dispatch:

jobs:
  deploy:
    env:
      IMAGE_NAME: ollama 

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Verify Dockerfile
        run: |
          ls -la
          test -f Dockerfile && echo "Dockerfile found" || exit 1

      - name: Deploy Service Using Custom Image
        run: |
          docker compose down
          docker compose up -d --build

      - name: Wait for Ollama to be ready
        run: |
          echo "Waiting for Ollama server to respond on port 7869..."
          timeout=120
          while ! curl -s http://z420:7869/ > /dev/null; do
            timeout=$((timeout - 1))
            if [ $timeout -le 0 ]; then
              echo "Error: Ollama server did not start in time."
              docker logs ollama
              exit 1
            fi
            sleep 1
          done
          echo "Ollama server is running."


      - name: Create Modelfile from Template
        env:
          BASE_MODEL: ${{ github.event.inputs.base_model }}
        run: |
          sudo apt update
          sudo apt-get install -y gettext
          
          envsubst < Modelfiles/Modelfile.quant_strategist_template > Modelfile.quant_strategist
          envsubst < Modelfiles/Modelfile.quant_dev_template > Modelfile.quant_dev 
        
          echo "Modelfiles created:"
          echo "quant_strategist: $(cat Modelfile.quant_strategist)"
          echo "quant_dev: $(cat Modelfile.quant_dev)"

      - name: Verify Modelfiles Mount
        run: |
          echo "Host Modelfiles:"
          ls -la ./Modelfiles/
          echo "Container Modelfiles:"
          docker exec ollama ls -la /Modelfiles/
          # If empty, copy manually as fallback
          if [ -z "$(docker exec ollama ls /Modelfiles/ 2>/dev/null)" ]; then
            echo "Volume mount failed, copying manually..."
            docker cp ./Modelfiles/. ollama:/Modelfiles/
          fi

      - name: Create From Modelfiles
        run: |
          docker exec ollama ls -laR /Modelfiles
          docker exec ollama ollama create quant_strategist -f /Modelfiles/Modelfile.quant_strategist
          docker exec ollama ollama create quant_dev -f /Modelfiles/Modelfile.quant_dev
        
      - name: Preload Models
        run: |
          echo "Pre-loading models" 
          
          curl --silent --show-error -X POST http://localhost:7869/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{
                  "model": "quant_strategist",
                  "messages": [
                    {"role" : "system", "content": "You are mute. You do not respond to any messages"}, 
                    {"role": "user", "content": "Initialize"},
                    {"role": "assistant", "content": ""},
                    {"role": "user", "content": "Initialize"}
                  ],
                  "stream": false
                }' &
          
          curl --silent --show-error -X POST http://localhost:7869/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{
                  "model": "quant_dev",
                  "messages": [
                    {"role" : "system", "content": "You are mute. You do not respond to any messages"},
                    {"role": "user", "content": "Initialize"},
                    {"role": "assistant", "content": ""},
                    {"role": "user", "content": "Initialize"}
                  ],
                  "stream": false
                }' &

          wait
          echo "Models pre-loaded successfully"