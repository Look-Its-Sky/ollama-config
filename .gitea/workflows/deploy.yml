name: Build and Deploy Ollama

on:
  push:
  workflow_dispatch:

jobs:
  deploy:
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Deploy Service 
        run: |
          hadtostop=true 
          echo "Stopping and removing existing Ollama container.... if any"

          docker stop ollama || hadtostop=false
          docker rm ollama || hadtostop=false 
          
          if [ "$hadtostop" = true ]; then
            echo "Ollama container stopped and removed successfully."
          else
            echo "No existing Ollama container to stop."
          fi

          docker run -d \
              --name ollama \
              --restart unless-stopped \
              --gpus all \
              --cpus="16" \
              -p 0.0.0.0:7869:11434 \
              -v /tank/llm/ollama:/root/.ollama:z \
              -e OLLAMA_KEEP_ALIVE=1h \
              -e OLLAMA_FLASH_ATTENTION=1 \
              -e OLLAMA_MAX_LOADED_MODELS=1 \
              -e OLLAMA_CONTEXT_LENGTH=1024 \
              -e OLLAMA_KV_CACHE_TYPE=q4_0 \
              ollama/ollama:latest

      - name: Wait for Ollama to be ready
        run: |
          echo "Waiting for Ollama server to respond on port 7869..."
          timeout=120

          while ! curl -s http://z420:7869/ > /dev/null; do
            timeout=$((timeout - 1))
            if [ $timeout -le 0 ]; then
              echo "Error: Ollama server did not start in time."
              docker logs ollama
              exit 1
            fi
            sleep 1
          done

          echo "Ollama server is running."

      - name: Create Modelfile from Template
        env:
          BASE_MODEL_STRATEGIST:  ${{ vars.BASE_MODEL_STRATEGIST }}
          BASE_MODEL_DEV:  ${{ vars.BASE_MODEL_DEV }}
        run: |
          sudo apt-get update
          sudo apt-get install -y gettext
          
          envsubst < Modelfiles/Modelfile.quant_strategist_template > Modelfile.quant_strategist
          envsubst < Modelfiles/Modelfile.quant_dev_template > Modelfile.quant_dev 

          docker exec ollama mkdir -p /Modelfiles

          echo "Modelfiles created:"
          echo "quant_strategist: $(cat Modelfile.quant_strategist)"
          echo "quant_dev: $(cat Modelfile.quant_dev)"

          docker cp Modelfiles ollama:/Modelfiles

      - name: Verify Modelfiles
        run: |
          echo "Host Modelfiles:"
          ls -la ./Modelfiles/

          echo "Container Modelfiles:"
          docker exec ollama ls -la /Modelfiles/
         
      - name: Pull Models 
        run: |
          docker exec ollama ollama pull "${{ vars.BASE_MODEL_STRATEGIST }}" > /dev/null
          docker exec ollama ollama pull "${{ vars.BASE_MODEL_DEV }}" > /dev/null

      - name: Create From Modelfiles
        run: |
          docker exec ollama ls -laR /Modelfiles

          docker exec ollama ollama rm quant_dev quant_strategist || true

          docker exec ollama ollama create quant_strategist -f /Modelfiles/Modelfile.quant_strategist
          docker exec ollama ollama create quant_dev -f /Modelfiles/Modelfile.quant_dev
        
      # - name: Preload Models
      #   env: 
      #     OLLAMA_SERVER: ${{ vars.OLLAMA_SERVER }}
      #   run: |
      #     echo "Pre-loading models" 

      #     curl --silent --show-error --fail -X POST http://$OLLAMA_SERVER/v1/chat/completions \
      #       -H "Content-Type: application/json" \
      #       -d '{
      #             "model": "quant_strategist",
      #             "messages": [
      #               {"role" : "system", "content": "You are mute. You do not respond to any messages"},
      #               {"role": "user", "content": "Initialize"},
      #               {"role": "assistant", "content": ""},
      #               {"role": "user", "content": "Initialize"}
      #             ],
      #             "stream": false
      #           }' > /dev/null &
      #     pids+=($!)

      #     # curl --silent --show-error --fail -X POST http://$OLLAMA_SERVER/v1/chat/completions \
      #     #   -H "Content-Type: application/json" \
      #     #   -d '{
      #     #         "model": "quant_dev",
      #     #         "messages": [
      #     #           {"role" : "system", "content": "You are mute. You do not respond to any messages"},
      #     #           {"role": "user", "content": "Initialize"},
      #     #           {"role": "assistant", "content": ""},
      #     #           {"role": "user", "content": "Initialize"}
      #     #         ],
      #     #         "stream": false
      #     #       }' > /dev/null &
      #     # pids+=($!)

      #     for pid in "${pids[@]}"; do
      #         wait "$pid"
      #     done

      #     wait
      #     echo "Models pre-loaded successfully"